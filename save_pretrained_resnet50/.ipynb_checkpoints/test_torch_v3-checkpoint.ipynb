{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys,time,platform\n",
    "#import openslide\n",
    "from PIL import Image\n",
    "#from tqdm import tqdm\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "#from torchvision import models as torch_models\n",
    "#from utils_preprocessing import *\n",
    "#import utils_color_norm\n",
    "#color_norm = utils_color_norm.macenko_normalizer()\n",
    "\n",
    "## check available device\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_extractor(nn.Module):\n",
    "    def __init__(self, layers=101):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "\n",
    "        x = self.resnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tile_00002_00083_00333_016.png' 'tile_00003_00069_00444_016.png'\n",
      " 'tile_00003_00070_00445_016.png' ... 'tile_00082_00074_10324_016.png'\n",
      " 'tile_00082_00075_10325_016.png' 'tile_00082_00076_10326_016.png']\n",
      "n_tiles: 5256\n"
     ]
    }
   ],
   "source": [
    "#path2tiles = \"/Volume/test_10tiles/\"\n",
    "path2tiles = \"/Volumes/TaiHoang5T6/LUSC_tiles/LUSC_00000_TCGA-18-3406-01Z-00-DX1/\"\n",
    "\n",
    "## collect selecting tile names within a slide folder\n",
    "tile_names = []\n",
    "for f in os.listdir(path2tiles):\n",
    "    if f.startswith(\"tile_\"):\n",
    "        tile_names.append(f)\n",
    "\n",
    "## alphabet sort\n",
    "tile_names = np.array(sorted(tile_names))\n",
    "print(tile_names)\n",
    "\n",
    "n_tiles = len(tile_names)\n",
    "print(\"n_tiles:\", n_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet_extractor().to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([5256, 3, 224, 224])\n",
      "time: 241.22160601615906\n"
     ]
    }
   ],
   "source": [
    "data_transform = transforms.Compose([transforms.Resize(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                          std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "##----------\n",
    "images = []\n",
    "for i_tile in range(n_tiles):\n",
    "    tile_name = tile_names[i_tile]\n",
    "    image = Image.open(f\"{path2tiles}{tile_name}\").convert('RGB')\n",
    "\n",
    "    image = data_transform(image).unsqueeze(0)\n",
    "    images.append(image)\n",
    "    \n",
    "images = torch.cat(images, dim=0)\n",
    "print(\"images.shape:\", images.shape)\n",
    "\n",
    "print(\"time:\", (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tiles_selected: 5256\n"
     ]
    }
   ],
   "source": [
    "n_tiles_selected = images.shape[0]\n",
    "print(\"n_tiles_selected:\", n_tiles_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(features_list): 83\n",
      "time: 514.7612488269806\n"
     ]
    }
   ],
   "source": [
    "##----------\n",
    "start_time = time.time()\n",
    "\n",
    "features_list = []\n",
    "for idx_start in range(0, n_tiles_selected, batch_size):\n",
    "    idx_end = idx_start + min(batch_size, n_tiles_selected - idx_start)\n",
    "\n",
    "    features = model(images[idx_start:idx_end].to(device))\n",
    "    \n",
    "    features_list.append(features.detach().cpu().numpy())\n",
    "\n",
    "print(\"len(features_list):\", len(features_list))\n",
    "\n",
    "print(\"time:\", (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
